test_mode: True
machine: "local"  # or "cluster"
device: "cuda"  # or "cpu"
anomaly_detection: False  # slows down the code
mode: gan  # 'gan' or 'regression' or 'figures' or 'embedding' or 'sampling' WIP
dataset_yaml_path: 'C:/Users/mikem/OneDrive/NYU/CSD/MCryGAN/configs/dataset/gan.yaml'
save_checkpoints: False # will do it always on cluster, only locally if True

# batching & convergence
grow_batch_size: True
min_batch_size: 10
max_batch_size: 10000
batch_growth_increment: 0.05 # fraction of batch size to grow by each epoch
max_epochs: 1000 # 0 epochs takes us straight to sampling/evaluation (only implemented for GAN)
history: 5000  # for convergence checks
gradient_norm_clip: 1

logger:
  run_name: dev
  mini_csp_frequency: 5
  sample_reporting_frequency: 5
  log_figures: True
  experiment_tag: dev

sample_steps: 20 # now for mini csp
sample_move_size: 0.001 # currently unused

extra_test_set_path: null  # todo reimplement
extra_test_period: 5 # how often to evaluate on extra test sets (hardcoded analysis per extra test set)

seeds:
  model: 0
  dataset: 0

# for reloading prior checkpoints
discriminator_path: null #'C:\Users\mikem\crystals\CSP_runs\models\cluster\best_discriminator_10413'
generator_path: null #'C:\Users\mikem\crystals\CSP_runs\models\cluster\best_generator_10440'
regressor_path: null  # 'C:\Users\mikem\crystals\CSP_runs\models\cluster\best_regressor_431'

discriminator_positional_noise: 0
generator_positional_noise: 0
regressor_positional_noise: 0

generate_sgs: ["P-1","P21/c","P212121","C2/c"] # null -> will generate in original sg.  'all' -> will randomly pick between all possibilities. []

# for GAN training
generator:
  canonical_conformer_orientation: 'standardized' # 'standardized' 'random'
  packing_target_noise: 0.05 # randn noise magnitude in the standardized basis
  train_vdw: True
  train_adversarially: True
  vdw_loss_func: inv  # null 'mse' or 'log' or 'inv'
  density_loss_func: l1  # 'l1' or 'mse'
  adversarial_loss_func: 'score'  #
  train_h_bond: False # (non-directional)
  similarity_penalty: 0

  #generator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 5.0E-4
    max_lr: 1.0E-3
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9 #0.9
    beta2: 0.999 #0.999
    weight_decay: 0.01
    convergence_eps: 0.00001
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.99

  model:
    num_fc_layers: 2
    fc_depth: 512
    activation: 'gelu'
    fc_dropout_probability: 0
    fc_norm_mode: layer

    prior: multivariate normal
    prior_dimension: 12

    conditioner:
      skinny_atomwise_features: False  # cut atom features down to bare minimum for conditioner input
      concat_mol_features: True  # exclusive with skinny features
      init_atom_embedding_dim: 5
      graph_convolution_layers: 4
      graph_filters: 64
      atom_embedding_size: 256
      output_dim: 256
      graph_norm: 'graph layer'
      num_spherical: 6
      num_radial: 36
      graph_convolution_cutoff: 3
      max_num_neighbors: 100
      radial_function: 'gaussian' # bessel or gaussian
      num_attention_heads: 1 # mikenet GATv2 mode only - must evenly divide graph filters
      graph_convolution_type: 'TransformerConv' #'TransformerConv' 'none' GATv2, schnet, or full message passing, or 'TransformerConv'
      add_spherical_basis: False # mikenet only
      add_torsional_basis: False
      graph_aggregator: max # combo, geometric max mean sum attention
      positional_embedding: 'sph3' # 'sph' or 'pos' or 'combo', for 'geometric' pooling

      num_fc_layers: 0
      fc_depth: 128
      activation: 'gelu'
      fc_dropout_probability: 0
      fc_norm_mode: null

discriminator:
  train_adversarially: True
  train_on_randn: True
  train_on_distorted: True
  distortion_magnitude: 0.1 # -1 for wide range test # noise for distorted generation

  # discriminator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-4
    max_lr: 2.0E-4
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.001
    convergence_eps: 0.00001
    training_period: 1
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.99

  model:
    crystal_convolution_type: 2 # 1 - counts inter and intramolecular the same, 2 - separates intermolecular
    graph_convolution_layers: 2
    graph_filters: 32
    atom_embedding_size: 64
    graph_norm: 'graph layer' # layer, graph, batch, graph layer
    num_spherical: 5
    num_radial: 32
    graph_convolution_cutoff: 6 # vdw direct force approx 6 angstrom
    max_num_neighbors: 100
    radial_function: 'bessel' # 'gaussian' or 'bessel'
    num_attention_heads: 4 # mikenet GATv2 or TransformerConv mode only - must evenly divide graph filters
    graph_convolution_type: 'TransformerConv' # 'full message passing' or schnet or 'GATv2' or 'TransformerConv'
    add_spherical_basis: False # mikenet only
    add_torsional_basis: False #
    graph_aggregator: combo

    num_fc_layers: 4
    fc_depth: 64
    activation: gelu
    fc_dropout_probability: 0
    fc_norm_mode: layer # batch, layer

regressor:
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-4
    max_lr: 1.0E-3
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9 #0.9
    beta2: 0.999 #0.999
    weight_decay: 0.01
    convergence_eps: 0.00001
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.975

  model:
    graph_convolution_type: TransformerConv
    graph_aggregator :  combo
    concat_mol_to_atom_features: True
    activation: gelu
    num_fc_layers: 4
    fc_depth: 128
    fc_norm_mode: null
    fc_dropout_probability: 0
    graph_node_norm: null
    graph_node_dropout: 0
    graph_message_norm: null
    graph_message_dropout: 0
    num_attention_heads: 4
    graph_message_depth: 128
    graph_node_dims: 256
    num_graph_convolutions: 4
    graph_embedding_depth: 256
    nodewise_fc_layers: 1
    num_radial: 12
    radial_function: bessel
    max_num_neighbors: 100
    convolution_cutoff: 6
    atom_type_embedding_dims: 5
