test_mode: False
machine: "cluster"
device: "cuda"
explicit_run_enumeration: True
skip_saving_and_loading: True
run_num: 1107
anomaly_detection: False
mode: gan # 'gan' or 'regression' or 'figures' or 'sampling'
gan_loss: standard # standard only
new_generation: True # new way of defining asymmetric unit
train_generator_packing: True
train_generator_vdw: True
train_generator_combo: False
vdw_loss_rescaling: mse
density_loss_rescaling: log
train_generator_h_bond: False # maximize h bonding (non-directional)
train_generator_adversarially: False
train_discriminator_adversarially: False
train_discriminator_on_randn: False
train_discriminator_on_noise: False
generator_noise_level: -1 # -1 for wide range test # noise for distorted generation
generator_similarity_penalty: 0
extra_test_period: 10
extra_test_evaluation: False
extra_test_set_paths: [ 'C:/Users/mikem/Desktop/CSP_runs/datasets/csd_coumarins_dataset']
  #[ 'C:/Users/mikem/Desktop/CSP_runs/datasets/blind_test_5_dataset',
                        #'C:/Users/mikem/Desktop/CSP_runs/datasets/blind_test_6_dataset',
                        #'C:/Users/mikem/Desktop/CSP_runs/datasets/blind_test_targets',]
d_model_path: null #'C:\Users\mikem\Desktop\CSP_runs\models/discriminator_404'
g_model_path: null #'C:\Users\mikem\Desktop\CSP_runs\models/cluster/generator_1009'

sample_steps: 100
sample_move_size: 0.01

seeds:
  model: 1
  dataset: 0

# opt - sweep details
wandb:
  sample_reporting_frequency: 5
  experiment_tag: generator_tests_feb1
  log_figures: True

# batching & convergence
accumulate_gradients: False
accumulate_batch_size: 1000
auto_batch_sizing: True
min_batch_size: 5
max_batch_size: 100
batch_growth_increment: 0.05
auto_batch_reduction: 0.6 # fraction of tested max batch size to actually start with (unequal graph sizes)
max_epochs: 10000 # 0 epochs takes us straight to evaluation mode
history: 1000
gradient_norm_clip: 1
lr_growth_lambda: 1.25
lr_shrink_lambda: 0.95

# dataset curation
target: packing # 'density' or 'packing' or 'volume'
dataset_length: 1000000
feature_richness: full # full or minimal
max_crystal_temperature: 1000
min_crystal_temperature: -100
max_num_atoms: 100
min_num_atoms: 8 # target XVI has only 9 heavy atoms
max_molecule_radius: 10 # angstroms
max_z_value: 18
min_z_value: 0
max_z_prime: 1
min_z_prime: 1
max_packing_coefficient: 0.85
min_packing_coefficient: 0.55
include_organic: True
include_organometallic: True
max_atomic_number: 100
exclude_missing_r_factor: False
exclude_disordered_crystals: True
exclude_polymorphs: True
exclude_nonstandard_settings: True #False
exclude_crystal_systems: null #['hexagonal','trigonal','rhombohedral']
exclude_blind_test_targets: True
include_sgs: ["P-1","P21/c","P212121","C2/c"] #["P-1", "P21/c", "P212121", "C2/c", "Pbca", "P21"]
include_pgs: null #[222,-1,2/m] #[222, -1, 2/m, 2, mm2, mmm, m]
generate_sgs: null #'P2' #'P21/c' # can only do one at a time right now
supercell_size: 5 # new method - should be very large, automatically pares down

# https://www.ruppweb.org/Xray/tutorial/enantio.htm non enantiogenic groups

# discriminator optimizer and model
discriminator:
  optimizer: "adamw"
  learning_rate: 0.00001
  max_lr: 0.00001
  lr_schedule: False
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.001
  convergence_eps: 0.00001
  training_period: 1
  positional_noise: 0 # scaling for the randn noise added to all atom positions

  crystal_convolution_type: 2 # 1 - counts inter and intramolecular the same, 2 - separates intermolecular
  graph_convolution_layers: 4
  graph_filters: 32
  atom_embedding_size: 64
  graph_norm: layer
  num_spherical: 5
  num_radial: 32
  graph_convolution_cutoff: 6 # vdw direct force approx 6 angstrom
  max_num_neighbors: 100
  radial_function: 'bessel' # mikenet only
  num_attention_heads: 1 # mikenet GATv2 mode only - must evenly divide graph filters
  graph_convolution: 'full message passing' # 'full message passing' or schnet or 'GATv2'
  add_spherical_basis: False # mikenet only
  add_torsional_basis: False #
  pooling: combo

  num_fc_layers: 2
  fc_depth: 64
  activation: leaky relu
  fc_dropout_probability: 0.1
  fc_norm_mode: layer

# generator optimizer and model
generator:
  optimizer: adamw
  learning_rate: 0.00001
  max_lr: 0.00005
  lr_schedule: True
  beta1: 0.9 #0.9
  beta2: 0.999 #0.999
  weight_decay: 0.01
  convergence_eps: 0.00001
  positional_noise: 0 # scaling for the randn noise added to all atom positions

  canonical_conformer_orientation: 'standardized' # 'standardized' 'random'
  graph_convolution_layers: 4
  graph_filters: 256
  atom_embedding_size: 512
  graph_norm: null
  num_spherical: 6
  num_radial: 12
  graph_convolution_cutoff: 3
  max_num_neighbors: 100
  radial_function: 'bessel' # mikenet only
  num_attention_heads: 4 # mikenet GATv2 mode only - must evenly divide graph filters
  graph_convolution: 'TransformerConv' # GATv2, schnet, or full message passing, or 'TransformerConv'
  add_spherical_basis: False # mikenet only
  add_torsional_basis: False
  pooling: geometric # combo, geometric (wip)

  conditioner_num_fc_layers: 4
  conditioner_fc_depth: 1028
  conditioner_activation: leaky relu
  conditioner_fc_dropout_probability: 0
  conditioner_fc_norm_mode: null

  num_fc_layers: 10
  fc_depth: 1028
  activation: leaky relu
  fc_dropout_probability: 0.1
  fc_norm_mode: layer

  prior: multivariate normal
  prior_dimension: 12
  conditional_modelling: True # false will likely not run at all
  conditioning_mode: graph model # molecule features or graph model



