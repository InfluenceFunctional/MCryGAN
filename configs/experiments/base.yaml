machine: "local"  # "local" or "cluster"
device: "cuda"  # "cuda" or "cpu"
anomaly_detection: False  # DEPRECATED slows down the code
mode: autoencoder_standalone_dev  # 'gan' for crystal generator AND/OR discriminator or 'regression' for molecule property prediction or 'figures' or 'autoencoder_standalone_dev' or 'search' WIP
dataset_name: 'test_dataset.pkl'  # dataset.pkl is large test_dataset.pkl is slow for faster prototyping
misc_dataset_name: 'misc_data_for_dataset.npy'  # contains necessary standardizations. Leave as-is in general
dataset_yaml_path: '/dataset/small_autoencoder.yaml'  # path within configs to .yaml which defines dataset filtering
extra_test_set_name: null #'test_blind_test_dataset.pkl'  #'acridin_dataset.pkl' #'test_blind_test_dataset.pkl'
save_checkpoints: False # will do it always on cluster, only locally if True

dataset:  # overwrite values from the dataset config
  max_dataset_length: 1000000

# batching & convergence
early_epochs_step_override: 100  # after how many steps to break an 'early' epoch
num_early_epochs: 10  # how many 'early' epochs should we have
grow_batch_size: True
min_batch_size: 2
max_batch_size: 500
batch_growth_increment: 0.05 # fraction of batch size to grow by each epoch
max_epochs: 100000 # 0 epochs takes us straight to sampling/evaluation (only implemented for GAN)
history: 1.0E7  # tail for convergence checks
gradient_norm_clip: 1
extra_test_period: 1 # # MUST BE MULTIPLE OF SAMPLE REPORTING FREQUENCY how often to evaluate on extra test sets (hardcoded analysis per extra test set)

logger:
  run_name: base
  experiment_tag: base
  mini_csp_frequency: 5  # how often to run CSP-style search WIP
  sample_reporting_frequency: 1  # how often to do detailed reporting with figures
  log_figures: True

seeds:
  model: 0
  dataset: 0

# for reloading prior checkpoints
model_paths:  # paths starting from the checkpoints path in the user config
  discriminator: null
  generator: null
  regressor: null
  autoencoder: null

discriminator_positional_noise: 0
generator_positional_noise: 0
regressor_positional_noise: 0
autoencoder_positional_noise: 0

generate_sgs: null  #["P-1","P21/c","P212121","C2/c"] # In which space groups should the generator sample new crystals
#null -> will generate in original sg.  'all' -> will randomly pick between all possibilities. []
supercell_size: 5  # how big of a supercell to build before paring down to crystal graph - 5 is a good value

# for GAN training
generator:
  # settings
  canonical_conformer_orientation: 'random' # 'standardized' or 'random'
  packing_target_noise: 0.05 # randn noise magnitude in the standardized basis
  train_vdw: True  # train using vdw overlap loss
  train_adversarially: True
  vdw_loss_func: inv  # null 'mse' or 'log' or 'inv'
  density_loss_func: l1  # 'l1' or 'mse'
  adversarial_loss_func: 'score'  # how to rescale the adversarial loss
  train_h_bond: False # train hydrogen bond matching score (non-directional and generally untested)
  similarity_penalty: 0  # scale for our various attempts to make it sample diversely - probably best not to use for now

  #generator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-4
    max_lr: 1.0E-3
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9 #0.9
    beta2: 0.999 #0.999
    weight_decay: 0.01
    convergence_eps: 0.00001
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.99
    use_plateau_scheduler: True

  model:
    num_fc_layers: 2
    fc_depth: 128
    activation: 'gelu'
    fc_dropout_probability: 0.1
    fc_norm_mode: layer
    conditioning_mode: 'first_layer' # only 'first_layer' has so far been implemented

    prior: multivariate normal  # only option that has been tested so far
    prior_dimension: 12  # NOTE this MUST be 12 for now
    radial_norm_factor: 5  # the factor by which to normalize the size of each molecule

    conditioner:
      graph_convolution_type: TransformerConv
      graph_aggregator: combo
      concat_mol_to_atom_features: False
      activation: gelu
      num_fc_layers: 4
      fc_depth: 256
      fc_norm_mode: layer
      fc_dropout_probability: 0.1
      graph_node_norm: 'graph layer'
      graph_node_dropout: 0.1
      graph_message_norm: null
      graph_message_dropout: 0
      num_attention_heads: 4
      graph_message_depth: 64
      graph_node_dims: 256
      num_graph_convolutions: 2
      graph_embedding_depth: 256
      nodewise_fc_layers: 1
      num_radial: 50
      radial_function: gaussian
      max_num_neighbors: 100
      convolution_cutoff: 6
      atom_type_embedding_dims: 5

discriminator:
  # settings
  train_adversarially: True  # train on samples from the generator
  train_on_randn: True  # train on gaussian-sampled cell params (poor)
  train_on_distorted: True  # train on true samples which have been distorted
  distortion_magnitude: -1 # -1 for wide range test # noise for distorted generation
  use_classification_loss: True
  use_rdf_distance_loss: True
  use_cell_distance_loss: False  # DO NOT USE - only meaningful when generating within the same space group

  # discriminator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-5
    max_lr: 2.0E-4
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.001
    convergence_eps: 0.00001
    training_period: 1
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.99
    use_plateau_scheduler: True

  model:
    graph_convolution_type: TransformerConv
    graph_aggregator:  combo
    concat_mol_to_atom_features: True
    activation: gelu
    num_fc_layers: 2
    fc_depth: 128
    fc_norm_mode: layer
    fc_dropout_probability: 0.25
    graph_node_norm: 'graph layer'
    graph_node_dropout: 0.25
    graph_message_norm: null
    graph_message_dropout: 0
    num_attention_heads: 4
    graph_message_depth: 128
    graph_node_dims: 256
    num_graph_convolutions: 4
    graph_embedding_depth: 256
    nodewise_fc_layers: 1
    num_radial: 32
    radial_function: bessel
    max_num_neighbors: 100
    convolution_cutoff: 6
    atom_type_embedding_dims: 5
    periodic_convolution_type: 'all_layers'  # or 'last_layer'


autoencoder:
  molecule_radius_normalization: 5  # factor by which to divide molecule radii
  random_fraction: 0  # fraction of samples to replace with random point clouds
  min_num_atoms: 2  # random point cloud # atoms
  max_num_atoms: 5
  init_sigma: 0.25  # starting value of the distribution width
  sigma_lambda: 0.99  # factor by which to anneal sigma
  sigma_threshold: 0.01  # reconstruction loss threshold at which to anneal sigma
  overlap_eps: 1.0E-3  # value of self overlap at which to stop annealing sigma
  independent_node_weights: False  # trainable nodewise weights in decoder (doesn't train well so far), else all the same
  node_weight_temperature: 10  # temperature for softmax for trainable nodewise weights
  type_distance_scaling: 0.5  # scaling factor to apply to atom types, effectively defining the 'size' of the typewise dimensions

  # discriminator optimizer and model
  optimizer:
    optimizer: adam
    init_lr: 1.0E-4
    encoder_init_lr: 1.0E-4
    decoder_init_lr: 1.0E-3
    max_lr: 1.0E-5
    min_lr: 1.0E-5
    lr_schedule: False
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.001
    convergence_eps: 0.00001
    training_period: 1
    lr_growth_lambda: 1.00
    lr_shrink_lambda: 0.995
    use_plateau_scheduler: False

  model:
    graph_convolution_type: TransformerConv
    graph_aggregator:  max
    embedding_depth: 512
    concat_mol_to_atom_features: True
    activation: gelu
    num_decoder_layers: 2
    decoder_norm_mode: batch
    decoder_dropout_probability: 0
    graph_node_norm: 'graph layer'
    graph_node_dropout: 0
    graph_message_dropout: 0
    num_attention_heads: 4
    graph_message_depth: 128
    graph_node_dims: 256
    num_graph_convolutions: 4
    graph_embedding_depth: 256
    nodewise_fc_layers: 1
    num_radial: 50
    radial_function: gaussian
    max_num_neighbors: 100
    convolution_cutoff: 2
    atom_type_embedding_dims: 5
    num_decoder_points: 256

regressor:
  # discriminator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-5
    max_lr: 2.0E-4
    min_lr: 1.0E-4
    lr_schedule: True
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.001
    convergence_eps: 0.00001
    training_period: 1
    lr_growth_lambda: 1.05
    lr_shrink_lambda: 0.99
    use_plateau_scheduler: True

  model:
    graph_convolution_type: TransformerConv
    graph_aggregator:  combo
    concat_mol_to_atom_features: True
    activation: gelu
    num_fc_layers: 2
    fc_depth: 128
    fc_norm_mode: layer
    fc_dropout_probability: 0.25
    graph_node_norm: 'graph layer'
    graph_node_dropout: 0.25
    graph_message_norm: null
    graph_message_dropout: 0
    num_attention_heads: 4
    graph_message_depth: 128
    graph_node_dims: 256
    num_graph_convolutions: 4
    graph_embedding_depth: 256
    nodewise_fc_layers: 1
    num_radial: 32
    radial_function: bessel
    max_num_neighbors: 100
    convolution_cutoff: 6
    atom_type_embedding_dims: 5

