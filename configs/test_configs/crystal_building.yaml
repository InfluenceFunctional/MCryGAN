machine: "local"  # or "cluster"
device: "cpu"  # or "cpu"
anomaly_detection: False  # slows down the code
mode: gan  # 'gan' or 'regression' or 'figures' or 'embedding' or 'sampling' WIP
dataset_name: 'test_dataset.pkl'
misc_dataset_name: 'misc_data_for_dataset.npy'
base_config_path: '/experiments/base.yaml'
dataset_yaml_path: '/dataset/full_gan_for_tests.yaml'
extra_test_set_name: null #'test_blind_test_dataset.pkl'  #'acridin_dataset.pkl' #'test_blind_test_dataset.pkl'
save_checkpoints: False # will do it always on cluster, only locally if True

dataset:
  max_dataset_length: 100

# batching & convergence
early_epochs_step_override: 100
num_early_epochs: 5
grow_batch_size: True
min_batch_size: 2
max_batch_size: 500
batch_growth_increment: 0.05 # fraction of batch size to grow by each epoch
max_epochs: 100000 # 0 epochs takes us straight to sampling/evaluation (only implemented for GAN)
history: 5000  # for convergence checks
gradient_norm_clip: 1
extra_test_period: 1 # # MUST BE MULTIPLE OF SAMPLE REPORTING FREQUENCY how often to evaluate on extra test sets (hardcoded analysis per extra test set)

logger:
  run_name: crystal_builder_test
  mini_csp_frequency: 5
  sample_reporting_frequency: 20
  log_figures: True
  experiment_tag: test

sample_steps: 100

seeds:
  model: 0
  dataset: 0

# for reloading prior checkpoints
model_paths:
  discriminator: null
  generator: null
  regressor: null
  autoencoder: null

discriminator_positional_noise: 0
generator_positional_noise: 0
regressor_positional_noise: 0
autoencoder_positional_noise: 0

generate_sgs: 'all'  #'["P21/c"] #["P-1","P21/c","P212121","C2/c"] # null -> will generate in original sg.  'all' -> will randomly pick between all possibilities. []
supercell_size: 5

# for GAN training
generator:
  canonical_conformer_orientation: 'random' # 'standardized' 'random'
  packing_target_noise: 0.05 # randn noise magnitude in the standardized basis
  train_vdw: True
  train_adversarially: True
  vdw_loss_func: inv  # null 'mse' or 'log' or 'inv'
  density_loss_func: l1  # 'l1' or 'mse'
  adversarial_loss_func: 'score'  #
  train_h_bond: False # (non-directional)
  similarity_penalty: 0

  #generator optimizer and model
  optimizer: null

  model: null


discriminator:
  train_adversarially: False
  train_on_randn: True
  train_on_distorted: True
  use_classification_loss: True
  use_rdf_distance_loss: True
  use_cell_distance_loss: True  # only meaninful when generating within the same space group
  distortion_magnitude: 0.1 # -1 for wide range test # noise for distorted generation

  # discriminator optimizer and model
  optimizer: null

  model: null

regressor:
  optimizer: null
  model: null

autoencoder:
  optimizer: null
  model: null